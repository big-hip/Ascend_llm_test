import torch
import torch.nn as nn
import io
import sys

from transformers import AutoConfig, AutoModelForCausalLM
import Dist_IR 

# --- LossWrapper ä¿æŒä¸å˜ ---
class LossWrapper(nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model
    
    def forward(self, input_ids, attention_mask, labels):
        out = self.model(
            input_ids=input_ids, 
            attention_mask=attention_mask,
            labels=labels, 
            use_cache=False, 
            return_dict=True
        )
        return out.loss

# --- æ¨¡å‹åˆå§‹åŒ– ---
print("Initializing Qwen model from configuration...")
model_path_or_identifier = './Qwen_8B_Base_local' 


config = AutoConfig.from_pretrained(model_path_or_identifier)
# except OSError:
#     print("Local path not found, using default Qwen/Qwen2-7B-beta (example)...")
#     config = AutoConfig.from_pretrained("Qwen/Qwen2-7B-beta")


# ================= ğŸ› ï¸ å¼ºåˆ¶ç˜¦èº«åŒºåŸŸ =================
print("ğŸ“‰ Overriding config to create a TINY model for debugging...")
config.hidden_size = 128               # åŸæ¥é€šå¸¸æ˜¯ 4096 -> å˜ç»†
config.intermediate_size = 512         # åŸæ¥é€šå¸¸æ˜¯ 11008+ -> MLPå˜å°
config.num_hidden_layers = 2           # åŸæ¥é€šå¸¸æ˜¯ 32 -> å˜æµ… (æœ€å…³é”®ï¼æå¤§å‡å°‘å›¾çš„é•¿åº¦)
config.num_attention_heads = 4         # åŸæ¥é€šå¸¸æ˜¯ 32 -> å¤´æ•°å‡å°‘
config.num_key_value_heads = 2         # ä¿æŒ GQA æ¯”ä¾‹ (å¯é€‰ï¼Œè®¾ä¸º num_attention_heads ä¸€æ ·ä¹Ÿå¯ä»¥)
config.max_position_embeddings = 128   # åºåˆ—é•¿åº¦ä¸Šé™å‡å°‘
# ====================================================
# config.attn_implementation = "eager"
# config._attn_implementation = "eager"
# config.use_cache = True 
config.output_hidden_states = False
config.output_attentions = False

if hasattr(config, "sliding_window"):
    config.sliding_window = None
if hasattr(config, "window_size"):
    config.window_size = None

model = AutoModelForCausalLM.from_config(config)
device = torch.device("cpu") 
model.to(device)
model.train() 

wrapped_model = LossWrapper(model).to(device)

# --- æ•°æ®å‡†å¤‡ ---
batch_size = 1
max_seq_length = 4
vocab_size = config.vocab_size
input_ids = torch.randint(0, vocab_size, (batch_size, max_seq_length), device=device)
labels = input_ids.clone()

# --- 4D Mask æ„å»º ---
causal_mask_bool = torch.tril(torch.ones((max_seq_length, max_seq_length), device=device))
causal_mask_bool = causal_mask_bool.view(1, 1, max_seq_length, max_seq_length).expand(batch_size, -1, -1, -1)
dtype = model.dtype if model.dtype is not None else torch.float32
min_value = torch.finfo(torch.float32).min if dtype == torch.float32 else -1e4
attention_mask = torch.zeros(causal_mask_bool.shape, dtype=dtype, device=device)
attention_mask = attention_mask.masked_fill(causal_mask_bool == 0, min_value)

# --- ä¼˜åŒ–å™¨ä¸ç¼–è¯‘ ---
optimizer = torch.optim.RMSprop(model.parameters(), lr=0.0001)
print("Capturing model graph with Dist_IR...")

graph_capture = Dist_IR.GraphCapture(wrapped_model, input_ids, attention_mask, labels)

try:
    compiled_model = graph_capture.compile()
    print("Graph capture and compilation complete.")
except Exception as e:
    print(f"Compilation failed: {e}")
    sys.exit(1)

# --- è®­ç»ƒå¾ªç¯ ---
loss = compiled_model(input_ids, attention_mask, labels)
optimizer.zero_grad()
loss.backward()
optimizer.step()
print("Optimization step complete.")

# --- ğŸ› ï¸ ä¿®æ”¹éƒ¨åˆ†: ä½¿ç”¨ Dist_IR.FxGraphDrawer ä¿å­˜ ---
print("Saving forward and backward graph modules...")



# def save_graph_with_drawer(gm, filename):
#     try:
#         # 2. å®ä¾‹åŒ– Drawer
#         g = str(gm.graph)
        
#         # 3. å†™å…¥æ–‡ä»¶ (å‡è®¾ str(g) è¿”å›å›¾çš„å†…å®¹)
#         with open(filename, 'w') as f:
#             f.write(g)
#         print(f"Saved {filename}")
#     except Exception as e:
#         print(f"Failed to save {filename}: {e}")
#         import traceback
#         traceback.print_exc()

from torch.fx.passes.graph_drawer import FxGraphDrawer

def save_dot_via_stdout(gm, filename, mode='w'):
    """
    å®Œå…¨å¤åˆ»ä½ æä¾›çš„é€»è¾‘ï¼š
    é€šè¿‡åŠ«æŒ sys.stdout æ¥æ•è· FxGraphDrawer çš„è¾“å‡ºï¼Œå¹¶å†™å…¥æ–‡ä»¶ã€‚
    """
    print(f"Saving DOT to {filename} via stdout capture...")
    
    try:
        # 1. å®ä¾‹åŒ– Drawer
        # ignore_getattr=True èƒ½è®©å›¾æ›´ç®€æ´
        g = FxGraphDrawer(gm, 'Qwen_Graph', ignore_getattr=True)
        
        # 2. ä½¿ç”¨ StringIO æ•è· print è¾“å‡º
        with io.StringIO() as buf:
            original_stdout = sys.stdout  # å¤‡ä»½åŸæ¥çš„ stdout
            sys.stdout = buf              # åŠ«æŒ stdout æŒ‡å‘ buffer
            
            # print ä¼šè°ƒç”¨ pydot å¯¹è±¡çš„ __str__ æ–¹æ³•ï¼Œå°† DOT å†…å®¹è¾“å‡ºåˆ° buf
            print(g.get_dot_graph())
            
            sys.stdout = original_stdout  # æ¢å¤åŸæ¥çš„ stdout
            output = buf.getvalue()       # è·å–æ•è·çš„å­—ç¬¦ä¸²å†…å®¹

        # 3. å†™å…¥æ–‡ä»¶
        # æ³¨æ„ï¼šä½ åŸæœ¬çš„ä»£ç ç”¨äº† 'a' (è¿½åŠ æ¨¡å¼)ï¼Œä½†åœ¨å•æ¬¡è„šæœ¬è¿è¡Œä¸­
        # ä¸ºäº†é¿å…æ–‡ä»¶å†…å®¹é‡å¤å †å ï¼Œè¿™é‡Œæˆ‘é»˜è®¤è®¾ä¸º 'w' (è¦†ç›–æ¨¡å¼)ã€‚
        # å¦‚æœä½ ç¡®å®éœ€è¦è¿½åŠ ï¼Œè°ƒç”¨æ—¶ä¼ å…¥ mode='a' å³å¯ã€‚
        with open(filename, mode) as file:
            file.write(output)
            
        print(f"âœ… Saved successfully: {len(output)} characters written.")

    except Exception as e:
        # æ¢å¤ stdout ä»¥é˜²å‡ºé”™åç»ˆç«¯æ²¡æœ‰ä»»ä½•è¾“å‡º
        if sys.stdout != sys.__stdout__:
            sys.stdout = sys.__stdout__
        print(f"âŒ Failed to save {filename}: {e}")
        import traceback
        traceback.print_exc()
# ä¿å­˜ FW å’Œ BW
save_dot_via_stdout(graph_capture.FW_gm, 'aten_module_FW_after.md')
save_dot_via_stdout(graph_capture.BW_gm, 'aten_module_BW_after.md')


print("Graphs saved successfully.")